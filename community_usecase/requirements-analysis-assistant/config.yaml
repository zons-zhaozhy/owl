# OWL Requirements Analysis System Configuration

# LLM Service Configuration
llm_provider: "deepseek"  # or "openai"
llm_model: "deepseek-chat"  # or "gpt-4-turbo-preview"
llm_temperature: 0.1
llm_max_tokens: 4000

# Default Model Settings
default_model: "deepseek-chat"
default_temperature: 0.7
default_max_tokens: 2000

# Logging Configuration
log_level: "DEBUG"
log_file: "logs/owl.log"
log_format: "<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
log_rotation: "1 day"
log_retention: "7 days"
log_compression: "zip"

# Agent Configuration
agents:
  requirements_extractor:
    name: "需求提取智能体"
    description: "从用户输入中提取需求"
    model: "deepseek-chat"
    temperature: 0.1
    max_tokens: 4000
    extra_config:
      prompt_template: "templates/prompts/extractor.txt"
      log_level: "DEBUG"
    
  requirements_analyzer:
    name: "需求分析智能体"
    description: "分析需求的可行性和依赖关系"
    model: "deepseek-chat"
    temperature: 0.1
    max_tokens: 4000
    extra_config:
      prompt_template: "templates/prompts/analyzer.txt"
      log_level: "DEBUG"
    
  quality_checker:
    name: "质量检查智能体"
    description: "检查需求的质量和完整性"
    model: "deepseek-chat"
    temperature: 0.1
    max_tokens: 4000
    extra_config:
      prompt_template: "templates/prompts/checker.txt"
      log_level: "DEBUG"
    
  document_generator:
    name: "文档生成智能体"
    description: "生成需求文档"
    model: "deepseek-chat"
    temperature: 0.1
    max_tokens: 4000
    extra_config:
      prompt_template: "templates/prompts/generator.txt"
      log_level: "DEBUG"

# System Configuration
templates:
  directory: "templates/prompts"
output_dir: "output"
template_dir: "templates"
workspace_dir: "workspace"

# Web Server Configuration
web_host: "127.0.0.1"
web_port: 8080
